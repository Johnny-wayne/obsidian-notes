
No aprendizado de máquina, uma das melhores formas de aprender mais sobre os dados é classificá-los com o que já se sabe. Você pode agrupar os dados com base em características semelhantes. Como as características são conhecidas, pode usar o aprendizado de máquina supervisionado. Um algoritmo de aprendizado de máquina supervisionado comum para classificação multiclasse é o k vizinho mais próximo, ou K-NN. Trata-se de um algoritmo de aprendizado de máquina baseado em instâncias, também conhecido como “aprendizado preguiçoso”. Com ele, a maior parte da computação ocorre logo antes da classificação dos dados. O aprendizado não é contínuo. Toda a computação é executada em uma grande instância. É como se você reservasse a energia para usar de uma só vez. O K-NN compara algo que você não sabe com o que você já tem. Portanto, você será recompensado de imediato pelo tamanho e qualidade dos dados de treinamento. A desvantagem é que requer muita energia computacional, e às vezes é difícil usar o K-NN em conjuntos de dados muito grandes. Veja desta forma: quando jovem, trabalhei em um abrigo para animais em Chicago. A atividade mais difícil, era classificar a raça de cada cão que chegava. Há centenas de raças de cães. Além disso, os cães não restringem seus parceiros de reprodução, e há muitas raças mistas. Cada vez que chegava um cão, o comparávamos com cães existentes, já classificados. Então, observávamos suas características: o formato do rosto ou a cor do pelo. De certo modo, o abrigo classificava o cão desconhecido comparando com o vizinho mais próximo. Obviamente, não era fácil dizer se o cão era um Boston Terrier ou um Buldogue Francês. Quanto mais parecido, maior a probabilidade de ser classificado. Outra forma de ver é minimizar a distância entre o cão desconhecido e as raças conhecidas. Se as características fossem muito parecidas, havia uma distância mínima entre o cão desconhecido e seu vizinho mais próximo. Minimizar a distância é uma parte essencial do k vizinho mais próximo. Quanto mais perto dos vizinhos mais próximos, maior a precisão. O modo mais comum de fazer isso é através da distância euclidiana, uma fórmula matemática sofisticada que ajuda a saber a distância entre pontos de dados. Agora, imagine que tenhamos milhões de cães e queiramos classificá-los por raça. Podemos usar duas características principais. Elas ajudarão a classificar os cães da mesma raça. Geralmente são chamadas de preditores. Usaremos o peso e o comprimento dos pelos. Vamos colocar essas duas características em um gráfico com eixos X e Y. O comprimento dos pelos no eixo Y e o peso no eixo X. Usaremos mil cães classificados neste conjunto de treinamento. Vamos colocá-los no gráfico com base em seu peso e comprimento do pelo. Agora colocaremos um cão desconhecido no gráfico. Ele não combina com nenhum outro cão, mas tem vários vizinhos próximos. Vamos usar um k igual a cinco.

![[K vizinho.png]]

Ou seja, faremos um círculo em torno desse cão não classificado e de seus cinco vizinhos mais próximos. Veja que, se a distância dos outros cães for menor, poderemos obter uma classificação bem mais precisa. Agora vamos examinar os cinco vizinhos mais próximos. Três deles são Pastores e dois, Huskies. Podemos estar um pouco confiantes para classificar o cão desconhecido como Pastor. Há também uma chance razoável de que seja um Husky. O k vizinho mais próximo é um algoritmo de aprendizado de máquina muito comum e poderoso. Isso porque ele pode fazer mais do que classificar cães, sendo muito usado em finanças, para descobrir as melhores ações e até prever seu futuro desempenho.

---
#machine-learning 